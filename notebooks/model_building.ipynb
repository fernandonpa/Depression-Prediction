{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Predicting Depression: Machine Learning Challenge**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In this part we mainly forcus on feature selection , feature creation , feature scalling , model building and evaluation part**\n",
    "\n",
    "*First of all let's import relevent libraries*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import optuna\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score , precision_score, recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Load and Process the Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_student = pd.read_csv('../data/processed_data/train_student.csv')\n",
    "df_test_student = pd.read_csv('../data/processed_data/test_student.csv')\n",
    "\n",
    "df_Wf = pd.read_csv('../data/processed_data/train_Wf.csv')\n",
    "df_test_Wf = pd.read_csv('../data/processed_data/test_Wf.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_s = df_student.drop(columns=['Depression']) \n",
    "y_s = df_student['Depression']\n",
    "\n",
    "X_w = df_Wf.drop(columns=['Depression'])\n",
    "y_w = df_Wf['Depression']\n",
    "\n",
    "x_train_s, x_test_s, y_train_s, y_test_s = train_test_split(X_s, y_s, test_size=0.2, random_state=0, stratify=y_s)\n",
    "X_train_w, X_test_w, y_train_w, y_test_w = train_test_split(X_w, y_w, test_size=0.2, random_state=0, stratify=y_w)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds= df_test_student[['id']]  \n",
    "dw= df_test_Wf[['id']]\n",
    "\n",
    "# Find common columns (excluding ID)\n",
    "common_cols_s = set(x_train_s.columns) & set(df_test_student.columns) - {'id'}\n",
    "common_cols_w = set(X_train_w.columns) & set(df_test_Wf.columns) - {'id'}\n",
    "\n",
    "# Keep only common columns\n",
    "x_train_s = x_train_s[list(common_cols_s)]\n",
    "x_test_s = x_test_s[list(common_cols_s)]\n",
    "df_test_student = df_test_student[list(common_cols_s)]\n",
    "\n",
    "x_train_w = X_train_w[list(common_cols_w)]\n",
    "x_test_w = X_test_w[list(common_cols_w)]\n",
    "df_test_Wf = df_test_Wf[list(common_cols_w)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Remove Constant, Quasi Constant and Duplicated Values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "const_filter_s = VarianceThreshold(threshold=0)\n",
    "const_filter_s.fit(x_train_s)\n",
    "x_train_sc = const_filter_s.transform(x_train_s)\n",
    "x_test_sc = const_filter_s.transform(x_test_s)\n",
    "\n",
    "df_test_sc = const_filter_s.transform(df_test_student)\n",
    "\n",
    "const_filter_w = VarianceThreshold(threshold=0)\n",
    "const_filter_w.fit(x_train_w)\n",
    "x_train_wc = const_filter_w.transform(x_train_w)\n",
    "x_test_wc = const_filter_w.transform(x_test_w)\n",
    "\n",
    "df_test_wc = const_filter_w.transform(df_test_Wf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "const_filter_s = VarianceThreshold(threshold=0.00002)\n",
    "const_filter_s.fit(x_train_sc)\n",
    "x_train_sq = const_filter_s.transform(x_train_sc)\n",
    "x_test_sq = const_filter_s.transform(x_test_sc)\n",
    "\n",
    "df_test_sq = const_filter_s.transform(df_test_sc)\n",
    "\n",
    "const_filter_w = VarianceThreshold(threshold=0.00005)\n",
    "const_filter_w.fit(x_train_wc)\n",
    "x_train_wq = const_filter_w.transform(x_train_wc)\n",
    "x_test_wq = const_filter_w.transform(x_test_wc)\n",
    "\n",
    "df_test_wq = const_filter_w.transform(df_test_wc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***No any duplicate values in the dataset then we don't remove duplicate Data***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Feature Scalling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scaler1 = StandardScaler()\n",
    "X_train_ss = scaler1.fit_transform(x_train_sq)\n",
    "X_test_ss = scaler1.transform(x_test_sq)\n",
    "\n",
    "df_test_ss = scaler1.transform(df_test_sq)\n",
    "\n",
    "\n",
    "scaler2 = StandardScaler()\n",
    "X_train_ws = scaler2.fit_transform(x_train_wq)\n",
    "X_test_ws = scaler2.transform(x_test_wq)\n",
    "\n",
    "df_test_ws = scaler2.transform(df_test_wq)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Linear Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score , precision_score, recall_score\n",
    "\n",
    "def traning_scores(y_act,y_pred):\n",
    "    acc = accuracy_score(y_act,y_pred)\n",
    "    f1 = f1_score(y_act,y_pred)\n",
    "    pre = precision_score(y_act,y_pred)\n",
    "    rec = recall_score(y_act,y_pred)\n",
    "    print(f'Traning Scores\\nAccuracy: {acc} \\nF1 Score: {f1} \\nPrecision: {pre} \\nRecall: {rec}')\n",
    "\n",
    "def validation_scores(y_act,y_pred):\n",
    "    acc = accuracy_score(y_act,y_pred)\n",
    "    f1 = f1_score(y_act,y_pred)\n",
    "    pre = precision_score(y_act,y_pred)\n",
    "    rec = recall_score(y_act,y_pred)\n",
    "    print(f'Testing Scores\\nAccuracy: {acc} \\nF1 Score: {f1} \\nPrecision: {pre} \\nRecall: {rec}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model For Working Frofettional**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traning Scores\n",
      "Accuracy: 0.9623756557323524 \n",
      "F1 Score: 0.7475721066082512 \n",
      "Precision: 0.8326284970722186 \n",
      "Recall: 0.6782827613621306\n",
      "Testing Scores\n",
      "Accuracy: 0.9626485568760611 \n",
      "F1 Score: 0.7504363001745201 \n",
      "Precision: 0.8317214700193424 \n",
      "Recall: 0.6836248012718601\n"
     ]
    }
   ],
   "source": [
    "lr_w = LogisticRegression(solver='saga', n_jobs=-1,C= 0.01, class_weight = None, fit_intercept= True, l1_ratio= 0.3, max_iter= 500, penalty= 'elasticnet', tol= 0.01)\n",
    "\n",
    "lr_w.fit(X_train_ws, y_train_w)\n",
    "\n",
    "y_train_pred = lr_w.predict(X_train_ws)\n",
    "traning_scores(y_train_w,y_train_pred)\n",
    "\n",
    "y_test_pred = lr_w.predict(X_test_ws)\n",
    "validation_scores(y_test_w,y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model for Student**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traning Scores\n",
      "Accuracy: 0.852815568176815 \n",
      "F1 Score: 0.8760336707828086 \n",
      "Precision: 0.8622527191765822 \n",
      "Recall: 0.8902622851974676\n",
      "Testing Scores\n",
      "Accuracy: 0.8517344602923049 \n",
      "F1 Score: 0.8752592592592593 \n",
      "Precision: 0.8607226107226107 \n",
      "Recall: 0.890295358649789\n"
     ]
    }
   ],
   "source": [
    "lr_s = LogisticRegression(solver='saga', n_jobs=-1,C= 0.01, class_weight = None, fit_intercept= True, l1_ratio= 0.3, max_iter= 500, penalty= 'elasticnet', tol= 0.01)\n",
    "lr_s.fit(X_train_ss, y_train_s)\n",
    "\n",
    "y_train_pred = lr_s.predict(X_train_ss)\n",
    "traning_scores(y_train_s,y_train_pred)\n",
    "\n",
    "y_test_pred = lr_s.predict(X_test_ss)\n",
    "validation_scores(y_test_s,y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***For Lodistic regression Working profettional Data work well but Student data has low prediction accuracy***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Na√Øve Bayes**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model for WF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traning Scores\n",
      "Accuracy: 0.7784985089571407 \n",
      "F1 Score: 0.40578102189781023 \n",
      "Precision: 0.2602329326292926 \n",
      "Recall: 0.9207632171723864\n",
      "Testing Scores\n",
      "Accuracy: 0.7787209960384833 \n",
      "F1 Score: 0.4057056003741377 \n",
      "Precision: 0.26027602760276025 \n",
      "Recall: 0.9194488606253313\n"
     ]
    }
   ],
   "source": [
    "nb_w = GaussianNB()\n",
    "nb_w.fit(X_train_ws, y_train_w)\n",
    "\n",
    "y_train_pred = nb_w.predict(X_train_ws)\n",
    "traning_scores(y_train_w, y_train_pred)\n",
    "\n",
    "y_test_pred = nb_w.predict(X_test_ws)\n",
    "validation_scores(y_test_w, y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model for Student**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traning Scores\n",
      "Accuracy: 0.4883987143926386 \n",
      "F1 Score: 0.2268795741849634 \n",
      "Precision: 0.9676503972758229 \n",
      "Recall: 0.12850467289719625\n",
      "Testing Scores\n",
      "Accuracy: 0.48265539707695015 \n",
      "F1 Score: 0.2119098712446352 \n",
      "Precision: 0.9634146341463414 \n",
      "Recall: 0.11904761904761904\n"
     ]
    }
   ],
   "source": [
    "nb_s = GaussianNB()\n",
    "nb_s.fit(X_train_ss, y_train_s)\n",
    "\n",
    "y_train_pred = nb_s.predict(X_train_ss)\n",
    "traning_scores(y_train_s, y_train_pred)\n",
    "\n",
    "y_test_pred = nb_s.predict(X_test_ss)\n",
    "validation_scores(y_test_s, y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Both two Datasets Naive base does not perfome well***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Decission Tree**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traning Scores\n",
      "Accuracy: 1.0 \n",
      "F1 Score: 1.0 \n",
      "Precision: 1.0 \n",
      "Recall: 1.0\n",
      "Testing Scores\n",
      "Accuracy: 0.9377040616429411 \n",
      "F1 Score: 0.6219286657859974 \n",
      "Precision: 0.6201264488935722 \n",
      "Recall: 0.6237413884472708\n"
     ]
    }
   ],
   "source": [
    "dt_w = DecisionTreeClassifier(random_state=42)\n",
    "dt_w.fit(X_train_ws, y_train_w)\n",
    "\n",
    "y_train_pred = dt_w.predict(X_train_ws)\n",
    "traning_scores(y_train_w, y_train_pred)\n",
    "\n",
    "y_test_pred = dt_w.predict(X_test_ws)\n",
    "validation_scores(y_test_w, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traning Scores\n",
      "Accuracy: 1.0 \n",
      "F1 Score: 1.0 \n",
      "Precision: 1.0 \n",
      "Recall: 1.0\n",
      "Testing Scores\n",
      "Accuracy: 0.7754886423666139 \n",
      "F1 Score: 0.8073727149116181 \n",
      "Precision: 0.8094516813086944 \n",
      "Recall: 0.805304400241109\n"
     ]
    }
   ],
   "source": [
    "dt_s = DecisionTreeClassifier(random_state=42)\n",
    "dt_s.fit(X_train_ss, y_train_s)\n",
    "\n",
    "y_train_pred = dt_s.predict(X_train_ss)\n",
    "traning_scores(y_train_s, y_train_pred)\n",
    "\n",
    "y_test_pred = dt_s.predict(X_test_ss)\n",
    "validation_scores(y_test_s, y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Random Forest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traning Scores\n",
      "Accuracy: 0.9999782329509589 \n",
      "F1 Score: 0.9998674794593162 \n",
      "Precision: 1.0 \n",
      "Recall: 0.9997349940373659\n",
      "Testing Scores\n",
      "Accuracy: 0.9556397196465108 \n",
      "F1 Score: 0.6709719082983533 \n",
      "Precision: 0.8586776859504133 \n",
      "Recall: 0.5506094329623742\n"
     ]
    }
   ],
   "source": [
    "rf_w = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rf_w.fit(X_train_ws, y_train_w)\n",
    "\n",
    "y_train_pred = rf_w.predict(X_train_ws)\n",
    "traning_scores(y_train_w, y_train_pred)\n",
    "\n",
    "y_test_pred = rf_w.predict(X_test_ws)\n",
    "validation_scores(y_test_w, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traning Scores\n",
      "Accuracy: 1.0 \n",
      "F1 Score: 1.0 \n",
      "Precision: 1.0 \n",
      "Recall: 1.0\n",
      "Testing Scores\n",
      "Accuracy: 0.8425779186476492 \n",
      "F1 Score: 0.867712340929269 \n",
      "Precision: 0.8523255813953489 \n",
      "Recall: 0.8836648583484027\n"
     ]
    }
   ],
   "source": [
    "rf_s = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rf_s.fit(X_train_ss, y_train_s)\n",
    "\n",
    "y_train_pred = rf_s.predict(X_train_ss)\n",
    "traning_scores(y_train_s, y_train_pred)\n",
    "\n",
    "y_test_pred = rf_s.predict(X_test_ss)\n",
    "validation_scores(y_test_s, y_test_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Random Forest is good but not as Logistic regression***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **XGBoost Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\npafe\\Desktop\\Machine Learning\\Depression Prediction\\env\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:27:14] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traning Scores\n",
      "Accuracy: 0.969471713719771 \n",
      "F1 Score: 0.8001140169600228 \n",
      "Precision: 0.8655565834104224 \n",
      "Recall: 0.743871737114085\n",
      "Testing Scores\n",
      "Accuracy: 0.9602977667493796 \n",
      "F1 Score: 0.7394285714285714 \n",
      "Precision: 0.8022318660880348 \n",
      "Recall: 0.6857445680975093\n"
     ]
    }
   ],
   "source": [
    "xgb_w = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "xgb_w.fit(X_train_ws, y_train_w)\n",
    "\n",
    "y_train_pred = xgb_w.predict(X_train_ws)\n",
    "traning_scores(y_train_w, y_train_pred)\n",
    "\n",
    "y_test_pred = xgb_w.predict(X_test_ws)\n",
    "validation_scores(y_test_w, y_test_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\npafe\\Desktop\\Machine Learning\\Depression Prediction\\env\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:27:35] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traning Scores\n",
      "Accuracy: 0.9139699731431339 \n",
      "F1 Score: 0.9272361659343115 \n",
      "Precision: 0.9163845134697483 \n",
      "Recall: 0.9383479047331926\n",
      "Testing Scores\n",
      "Accuracy: 0.8376474731466808 \n",
      "F1 Score: 0.8625521765056648 \n",
      "Precision: 0.8533923303834808 \n",
      "Recall: 0.8719107896323086\n"
     ]
    }
   ],
   "source": [
    "xgb_s = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "xgb_s.fit(X_train_ss, y_train_s)\n",
    "\n",
    "y_train_pred = xgb_s.predict(X_train_ss)\n",
    "traning_scores(y_train_s, y_train_pred)\n",
    "\n",
    "y_test_pred = xgb_s.predict(X_test_ss)\n",
    "validation_scores(y_test_s, y_test_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***XGBoost Clssifier also works well for the dataset***\n",
    "\n",
    "***Now let's assemble multiple model to get best accurate predictions***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Hyperparameter Tuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    C = trial.suggest_loguniform('C', 0.0001, 10)  # Regularization strength\n",
    "    l1_ratio = trial.suggest_float('l1_ratio', 0.1, 0.9)  # ElasticNet mix ratio\n",
    "    max_iter = trial.suggest_int('max_iter', 100, 1000, step=100)  # Iterations\n",
    "    tol = trial.suggest_loguniform('tol', 0.0001, 0.1)  # Tolerance\n",
    "\n",
    "    # Define the model\n",
    "    model = LogisticRegression(solver='saga', n_jobs=-1, penalty='elasticnet',\n",
    "                               fit_intercept=True, C=C, l1_ratio=l1_ratio,\n",
    "                               max_iter=max_iter, tol=tol)\n",
    "\n",
    "    # Perform cross-validation\n",
    "    accuracy = cross_val_score(model, X_train_ws, y_train_w, cv=5, scoring='accuracy', n_jobs=-1).mean()\n",
    "    \n",
    "    return accuracy  # Maximize accuracy\n",
    "\n",
    "# Create Optuna study\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# Get best hyperparameters\n",
    "best_params = study.best_params\n",
    "print(\"Best Hyperparameters (Optuna):\", best_params)\n",
    "\n",
    "# Train model with best hyperparameters\n",
    "best_model = LogisticRegression(solver='saga', n_jobs=-1, penalty='elasticnet', fit_intercept=True, **best_params) # change Model and grid for others \n",
    "best_model.fit(X_train_ws, y_train_w)\n",
    "\n",
    "# Train and evaluate the best model\n",
    "y_train_pred = best_model.predict(X_train_ws)\n",
    "traning_scores(y_train_w, y_train_pred)\n",
    "\n",
    "y_test_pred = best_model.predict(X_test_ws)\n",
    "validation_scores(y_test_w, y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Model Assemble**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Scores:\n",
      "Traning Scores\n",
      "Accuracy: 0.9633987070372869 \n",
      "F1 Score: 0.7493478422896326 \n",
      "Precision: 0.8563884156729131 \n",
      "Recall: 0.6660924870809594\n",
      "Validation Scores:\n",
      "Testing Scores\n",
      "Accuracy: 0.9629532889295198 \n",
      "F1 Score: 0.7468015471585837 \n",
      "Precision: 0.8514246947082768 \n",
      "Recall: 0.6650768415474297\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Define individual models\n",
    "lr = LogisticRegression(solver='saga', n_jobs=-1,C= 0.01, class_weight = None, fit_intercept= True, l1_ratio= 0.3, max_iter= 500, penalty= 'elasticnet', tol= 0.01, random_state=42)\n",
    "rf = RandomForestClassifier(n_estimators=200, max_depth=15, random_state=42, n_jobs=-1)\n",
    "lgbm = LGBMClassifier(n_estimators=200, learning_rate=0.05, max_depth=10, random_state=42, n_jobs=-1)\n",
    "\n",
    "# Create a Voting Classifier (Soft Voting for better probability estimates)\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('lr', lr), ('rf', rf), ('lgbm', lgbm)],\n",
    "    voting='soft', \n",
    "    weights=[2.9853169041728673, 0.5770399926666891, 0.5075508446786692], # 'hard' for majority voting, 'soft' for probability averaging\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Train the ensemble model\n",
    "voting_clf.fit(X_train_ws, y_train_w)\n",
    "\n",
    "# Predict on training data\n",
    "y_train_pred = voting_clf.predict(X_train_ws)\n",
    "print(\"Training Scores:\")\n",
    "traning_scores(y_train_w, y_train_pred)\n",
    "\n",
    "# Predict on test data\n",
    "y_test_pred = voting_clf.predict(X_test_ws)\n",
    "print(\"Validation Scores:\")\n",
    "validation_scores(y_test_w, y_test_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parameter Tuning For Best weights**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import optuna\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define individual models with new names\n",
    "log_reg = LogisticRegression(solver='saga', n_jobs=-1, C=0.01, class_weight=None, fit_intercept=True, \n",
    "                             l1_ratio=0.3, max_iter=500, penalty='elasticnet', tol=0.01, random_state=42)\n",
    "rand_forest = RandomForestClassifier(n_estimators=200, max_depth=15, random_state=42, n_jobs=-1)\n",
    "light_gbm = LGBMClassifier(n_estimators=200, learning_rate=0.05, max_depth=10, random_state=42, n_jobs=-1)\n",
    "\n",
    "# Define objective function for optimization\n",
    "def objective(trial):\n",
    "    w1 = trial.suggest_float(\"w1\", 0.5, 3.0)  # Logistic Regression weight\n",
    "    w2 = trial.suggest_float(\"w2\", 0.5, 3.0)  # Random Forest weight\n",
    "    w3 = trial.suggest_float(\"w3\", 0.5, 3.0)  # LightGBM weight\n",
    "\n",
    "    ensemble_clf = VotingClassifier(\n",
    "        estimators=[('log_reg', log_reg), ('rand_forest', rand_forest), ('light_gbm', light_gbm)],\n",
    "        voting='soft',\n",
    "        weights=[w1, w2, w3],\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    # Train model\n",
    "    ensemble_clf.fit(X_train_ws, y_train_w)\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    y_val_pred = ensemble_clf.predict(X_test_ws)\n",
    "    accuracy = accuracy_score(y_test_w, y_val_pred)\n",
    "    \n",
    "    return accuracy  # Maximizing accuracy\n",
    "\n",
    "# Run Optuna optimization\n",
    "study = optuna.create_study(direction=\"maximize\")  # Maximize accuracy\n",
    "study.optimize(objective, n_trials=30)  # Try 30 different weight combinations\n",
    "\n",
    "# Get best weights\n",
    "best_weights = [study.best_params[\"w1\"], study.best_params[\"w2\"], study.best_params[\"w3\"]]\n",
    "print(f\"Best Weights Found: {best_weights}\")\n",
    "\n",
    "# Train final model with best weights\n",
    "best_ensemble_clf = VotingClassifier(\n",
    "    estimators=[('log_reg', log_reg), ('rand_forest', rand_forest), ('light_gbm', light_gbm)],\n",
    "    voting='soft',\n",
    "    weights=best_weights,  #Best Weights Found: [2.9853169041728673, 0.5770399926666891, 0.5075508446786692]\n",
    "    n_jobs=-1\n",
    ")\n",
    "best_ensemble_clf.fit(X_train_ws, y_train_w)\n",
    "\n",
    "# Predict on training data\n",
    "y_train_pred = best_ensemble_clf.predict(X_train_ws)\n",
    "print(\"Training Scores:\")\n",
    "traning_scores(y_train_w, y_train_pred)\n",
    "\n",
    "# Predict on test data\n",
    "y_test_pred = best_ensemble_clf.predict(X_test_ws)\n",
    "print(\"Validation Scores:\")\n",
    "validation_scores(y_test_w, y_test_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```Python\n",
    "\n",
    "Best Weights Found: [2.9640932516597274, 0.5310683971855825, 2.8126479186454434]\n",
    "\n",
    "Training Scores:\n",
    "Traning Scores\n",
    "Accuracy: 0.9642585054744128 \n",
    "F1 Score: 0.7565243179122183 \n",
    "Precision: 0.8587779835044606 \n",
    "Recall: 0.6760302106797403\n",
    "\n",
    "Validation Scores:\n",
    "Testing Scores\n",
    "Accuracy: 0.9624744242740847 \n",
    "F1 Score: 0.7446682464454977 \n",
    "Precision: 0.8441907320349228 \n",
    "Recall: 0.6661367249602543\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Scores:\n",
      "Traning Scores\n",
      "Accuracy: 0.8721437062475235 \n",
      "F1 Score: 0.892468340368807 \n",
      "Precision: 0.8772019216771 \n",
      "Recall: 0.9082755501959602\n",
      "Validation Scores:\n",
      "Testing Scores\n",
      "Accuracy: 0.8496214122204614 \n",
      "F1 Score: 0.873293768545994 \n",
      "Precision: 0.8600233781414378 \n",
      "Recall: 0.8869801084990958\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Define individual models\n",
    "lr = LogisticRegression(solver='saga', n_jobs=-1,C= 0.01, class_weight = None, fit_intercept= True, l1_ratio= 0.3, max_iter= 500, penalty= 'elasticnet', tol= 0.01, random_state=42)\n",
    "rf = RandomForestClassifier(n_estimators=200, max_depth=15, random_state=42, n_jobs=-1)\n",
    "lgbm = LGBMClassifier(n_estimators=200, learning_rate=0.05, max_depth=10, random_state=42, n_jobs=-1)\n",
    "\n",
    "# Create a Voting Classifier (Soft Voting for better probability estimates)\n",
    "best_voting_clf = VotingClassifier(\n",
    "    estimators=[('lr', lr), ('rf', rf), ('lgbm', lgbm)],\n",
    "    voting='soft',\n",
    "    weights=[1.9432688188611906, 0.8690600198005576, 1.2477096350059103], \n",
    "    n_jobs=-1\n",
    ")\n",
    "best_voting_clf.fit(X_train_ss, y_train_s)\n",
    "\n",
    "# Predict on training data\n",
    "y_train_pred = best_voting_clf.predict(X_train_ss)\n",
    "print(\"Training Scores:\")\n",
    "traning_scores(y_train_s, y_train_pred)\n",
    "\n",
    "# Predict on test data\n",
    "y_test_pred = best_voting_clf.predict(X_test_ss)\n",
    "print(\"Validation Scores:\")\n",
    "validation_scores(y_test_s, y_test_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parameter Tuning for Best Weights**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define individual models\n",
    "lr = LogisticRegression(solver='saga', n_jobs=-1, C=0.01, class_weight=None, fit_intercept=True, \n",
    "                        l1_ratio=0.3, max_iter=500, penalty='elasticnet', tol=0.01, random_state=42)\n",
    "rf = RandomForestClassifier(n_estimators=200, max_depth=15, random_state=42, n_jobs=-1)\n",
    "lgbm = LGBMClassifier(n_estimators=200, learning_rate=0.05, max_depth=10, random_state=42, n_jobs=-1)\n",
    "\n",
    "# Define objective function for optimization\n",
    "def objective(trial):\n",
    "    w1 = trial.suggest_float(\"w1\", 0.5, 3.0)  # Logistic Regression weight\n",
    "    w2 = trial.suggest_float(\"w2\", 0.5, 3.0)  # Random Forest weight\n",
    "    w3 = trial.suggest_float(\"w3\", 0.5, 3.0)  # LightGBM weight\n",
    "\n",
    "    voting_clf = VotingClassifier(\n",
    "        estimators=[('lr', lr), ('rf', rf), ('lgbm', lgbm)],\n",
    "        voting='soft',\n",
    "        weights=[w1, w2, w3],\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    # Train model\n",
    "    voting_clf.fit(X_train_ss, y_train_s)\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    y_val_pred = voting_clf.predict(X_test_ss)\n",
    "    accuracy = accuracy_score(y_test_s, y_val_pred)\n",
    "    \n",
    "    return accuracy  # Maximizing accuracy\n",
    "\n",
    "# Run Optuna optimization\n",
    "study = optuna.create_study(direction=\"maximize\")  # Maximize accuracy\n",
    "study.optimize(objective, n_trials=50)  # Try 30 different weight combinations\n",
    "\n",
    "# Get best weights\n",
    "best_weights = [study.best_params[\"w1\"], study.best_params[\"w2\"], study.best_params[\"w3\"]]\n",
    "print(f\"Best Weights Found: {best_weights}\")\n",
    "\n",
    "# Train final model with best weights\n",
    "best_voting_clf = VotingClassifier(\n",
    "    estimators=[('lr', lr), ('rf', rf), ('lgbm', lgbm)],\n",
    "    voting='soft',\n",
    "    weights=best_weights,\n",
    "    n_jobs=-1\n",
    ")\n",
    "best_voting_clf.fit(X_train_ss, y_train_s)\n",
    "\n",
    "# Predict on training data\n",
    "y_train_pred = best_voting_clf.predict(X_train_ss)\n",
    "print(\"Training Scores:\")\n",
    "traning_scores(y_train_s, y_train_pred)\n",
    "\n",
    "# Predict on test data\n",
    "y_test_pred = best_voting_clf.predict(X_test_ss)\n",
    "print(\"Validation Scores:\")\n",
    "validation_scores(y_test_s, y_test_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Python\n",
    "Best Weights Found: [1.9432688188611906, 0.8690600198005576, 1.2477096350059103]\n",
    "\n",
    "Training Scores:\n",
    "Traning Scores\n",
    "Accuracy: 0.8717914850526131 \n",
    "F1 Score: 0.8922359558877951 \n",
    "Precision: 0.8764722989675731 \n",
    "Recall: 0.9085770274344287\n",
    "\n",
    "Validation Scores:\n",
    "Testing Scores\n",
    "Accuracy: 0.8506779362563832 \n",
    "F1 Score: 0.8741839762611276 \n",
    "Precision: 0.8609000584453536 \n",
    "Recall: 0.8878842676311031\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Save the Predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_student = lr_s.predict(df_test_ss)\n",
    "y_pred_wf = lr_w.predict(df_test_ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['Depression'] = y_pred_student\n",
    "dw['Depression'] = y_pred_wf\n",
    "\n",
    "df_final = pd.concat([dw,ds])  # Merge predictions\n",
    "df_final = df_final.sort_values(by='id')  # Sort to match original order\n",
    "\n",
    "df_final.to_csv('../predictions/lr_5.csv', index=False)  # Save to file\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
